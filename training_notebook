{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":27923,"databundleVersionId":3495119,"sourceType":"competition"},{"sourceId":3521209,"sourceType":"datasetVersion","datasetId":2089184}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/quntrnhongng/uwmgi-unet-training-pytorch-lightning?scriptVersionId=200168779\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install -q segmentation_models_pytorch\n!pip install lightning\n!pip install hydra-core","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:07:07.653964Z","iopub.execute_input":"2024-10-09T11:07:07.654294Z","iopub.status.idle":"2024-10-09T11:07:58.329315Z","shell.execute_reply.started":"2024-10-09T11:07:07.654268Z","shell.execute_reply":"2024-10-09T11:07:58.327362Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.5.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.3.post0)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.4.0.post0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.4)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.3.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.4.0\nCollecting hydra-core\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting omegaconf<2.4,>=2.2 (from hydra-core)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting antlr4-python3-runtime==4.9.* (from hydra-core)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from hydra-core) (21.3)\nRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.10/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->hydra-core) (3.1.1)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=201d87fe40b003fa1c4802188f60b5b27af1242f010caf4c3ff3001a3435c8e7\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built antlr4-python3-runtime\nInstalling collected packages: antlr4-python3-runtime, omegaconf, hydra-core\nSuccessfully installed antlr4-python3-runtime-4.9.3 hydra-core-1.3.2 omegaconf-2.3.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Standard libraries\nimport math\nimport os\nimport random\nimport time\nimport urllib.request\nfrom functools import partial\nfrom urllib.error import HTTPError\nfrom types import SimpleNamespace\nimport collections\nfrom typing import Any, Dict, List, Union\nimport importlib\nfrom glob import glob\n\n# Plotting\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib_inline.backend_inline\nimport seaborn as sns\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom PIL import Image\n\n# Libraries for ML\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.model_selection import *\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import random_split, Dataset, DataLoader, Subset\nfrom torchvision.transforms import v2\nimport torchmetrics\n\n# Torchvision\nimport torchvision\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:08:15.971511Z","iopub.execute_input":"2024-10-09T11:08:15.971905Z","iopub.status.idle":"2024-10-09T11:08:24.760853Z","shell.execute_reply.started":"2024-10-09T11:08:15.971864Z","shell.execute_reply":"2024-10-09T11:08:24.760026Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# PyTorch Lightning\nimport lightning as L\nfrom lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\nfrom lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, Callback","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:08:24.762562Z","iopub.execute_input":"2024-10-09T11:08:24.763408Z","iopub.status.idle":"2024-10-09T11:08:25.078106Z","shell.execute_reply.started":"2024-10-09T11:08:24.763372Z","shell.execute_reply":"2024-10-09T11:08:25.077279Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from omegaconf import DictConfig, OmegaConf\nimport timm\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:08:25.079229Z","iopub.execute_input":"2024-10-09T11:08:25.07952Z","iopub.status.idle":"2024-10-09T11:08:27.553198Z","shell.execute_reply.started":"2024-10-09T11:08:25.079495Z","shell.execute_reply":"2024-10-09T11:08:27.552393Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Logging","metadata":{}},{"cell_type":"code","source":"import logging\nimport logging.config\n\n# 5 levels: DEBUG, INFO, WARNING, ERROR, CRITICAL\nlog_config = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        },\n        'detailed': {\n            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)',\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'level': 'DEBUG',\n            'formatter': 'default',\n            'stream': 'ext://sys.stdout',\n        },\n        'file': {\n            'class': 'logging.FileHandler',\n            'level': 'ERROR',\n            'formatter': 'detailed',\n            'filename': 'app.log',\n            'mode': 'w',\n        },\n    },\n    'loggers': {\n#         '': {  # root logger\n#             'level': 'DEBUG',\n#             'handlers': ['console'],\n#         },\n        'my_logger': {  # the new_logger is a child of the root logger \n            'level': 'DEBUG',\n            'handlers': ['console'],\n            # If 'propagate' is set to True, all messages loged to new_logger \n            # are loged to its parent as well\n            'propagate': False, \n        },\n    },\n}\n\ndef setup_logging(log_config):\n#     logger = logging.getLogger('test_logger')\n#     if logger.hasHandlers():\n#         logger.handlers.clear()\n    logging.config.dictConfig(log_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:20.10264Z","iopub.execute_input":"2024-10-09T11:10:20.103371Z","iopub.status.idle":"2024-10-09T11:10:20.111681Z","shell.execute_reply.started":"2024-10-09T11:10:20.103335Z","shell.execute_reply":"2024-10-09T11:10:20.110746Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"setup_logging(log_config)\n\n# New logger defined in the new configuration\nmy_logger = logging.getLogger('my_logger')\nmy_logger.debug('This is a debug message from the my_logger.')\nmy_logger.info('This is an info message from the my_logger.')\nmy_logger.warning('This is a warning message from the my_logger.')\nmy_logger.error('This is an error message from the my_logger.')\nmy_logger.critical('This is a critical message from the my_logger.')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:20.451032Z","iopub.execute_input":"2024-10-09T11:10:20.4519Z","iopub.status.idle":"2024-10-09T11:10:20.467529Z","shell.execute_reply.started":"2024-10-09T11:10:20.451862Z","shell.execute_reply":"2024-10-09T11:10:20.466649Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2024-10-09 11:10:20,460 - my_logger - DEBUG - This is a debug message from the my_logger.\n2024-10-09 11:10:20,461 - my_logger - INFO - This is an info message from the my_logger.\n2024-10-09 11:10:20,461 - my_logger - WARNING - This is a warning message from the my_logger.\n2024-10-09 11:10:20,462 - my_logger - ERROR - This is an error message from the my_logger.\n2024-10-09 11:10:20,463 - my_logger - CRITICAL - This is a critical message from the my_logger.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Wandb ","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n\n!wandb login $secret_value","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:21.372174Z","iopub.execute_input":"2024-10-09T11:10:21.373049Z","iopub.status.idle":"2024-10-09T11:10:24.718882Z","shell.execute_reply.started":"2024-10-09T11:10:21.373014Z","shell.execute_reply":"2024-10-09T11:10:24.717698Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:24.720727Z","iopub.execute_input":"2024-10-09T11:10:24.72116Z","iopub.status.idle":"2024-10-09T11:10:24.726378Z","shell.execute_reply.started":"2024-10-09T11:10:24.72112Z","shell.execute_reply":"2024-10-09T11:10:24.725441Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"dict_cfg = {'general': {'workspace': 'kaggle', 'project_name': 'mri_seg'},\n       'training': {\n           'lr': 2e-3, 'metric': 'val_GeneralizedDiceScore',\n           'seed': 42, 'debug': False,\n           'mode': 'max', 'save_dir': '/kaggle/working',\n           'save_name': 'training_4', \n           'pretrained': False, 'pretrained_fname': ''\n       },\n       'optimizers': [\n            {\n                'class_name': 'torch.optim.AdamW',\n                'params': {'lr': '${training.lr}'}\n            },  \n       ],\n       'schedulers': [\n            {\n                'scheduler': {\n                    'class_name': 'torch.optim.lr_scheduler.CosineAnnealingLR',\n                    'params': {'T_max': 25, 'eta_min': 1e-5}\n                },\n                'interval': 'epoch',\n            }\n       ],\n       'data': {\n           'full_image_path': '/kaggle/working/full_image.csv',\n           'test_image_path': '/kaggle/working/test_image.csv',\n           'num_workers': 4, 'batch_size': 128, \n           'in_channels_num': 3, 'img_h': 224, 'img_w': 224,\n           'classes': ['Large Bowel', 'Small Bowel', 'Stomach'],\n           'classes_num': 3,\n           'splits_num': 5, 'fold_idx': 0\n       },\n      \n      'metrics':[\n          {\n              'class_name': 'torchmetrics.segmentation.GeneralizedDiceScore', \n               'params': {\n                   'num_classes': '${data.classes_num}',\n                   'weight_type': 'linear'\n               }\n          }\n      ]}","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:27.164546Z","iopub.execute_input":"2024-10-09T11:10:27.165863Z","iopub.status.idle":"2024-10-09T11:10:27.174502Z","shell.execute_reply.started":"2024-10-09T11:10:27.165816Z","shell.execute_reply":"2024-10-09T11:10:27.173489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dict_cfg['augmentation'] = {\n    'train': {\n       'class_name': 'albumentations.Compose',\n       'params':{\n           'transforms': [\n               {\n                   'class_name': 'albumentations.Resize'\n                   , 'params': {\n                       'height': '${data.img_h}', 'width': '${data.img_w}', 'p': 1.0}\n               },\n               # Geometric transforms 50%\n               {\n                   'class_name': 'albumentations.OneOf',\n                   'params': {\n                       'transforms': [\n                           {\n                               'class_name': 'albumentations.Affine', \n                               'params': {\n                                   'scale': [-0.05, 0.05], \n                                   'translate_percent': [-0.05, 0.05],\n                                   'rotate': [-10, 10]}\n                           },\n                           {\n                               'class_name': 'albumentations.HorizontalFlip',\n                               'params': {}\n                           },\n                       ],\n                       'p': 0.5}\n               },\n               # Color related transforms 20%\n               {\n                   'class_name': 'albumentations.OneOf',\n                   'params': {\n                       'transforms': [\n                           {\n                               'class_name': 'albumentations.RandomBrightnessContrast', \n                               'params': {\n                                   'brightness_limit': 0.2, 'contrast_limit': 0.2\n                               }\n                           },\n                       ],\n                       'p': 0.2}\n               },\n               # Blur/Noise related transforms 20%\n               {\n                   'class_name': 'albumentations.OneOf',\n                   'params': {\n                       'transforms': [\n                           {\n                               'class_name': 'albumentations.GaussNoise',\n                               'params': {'var_limit': [10.0, 50.0]}\n                           },\n                       ],\n                       'p': 0.2}\n               },\n           ]},\n    },\n    'valid': {\n       'class_name': 'albumentations.Compose',\n       'params':{\n           'transforms': [\n               {'class_name': 'albumentations.Resize', 'params': {'height': '${data.img_h}', 'width': '${data.img_w}', 'p': 1.0}},\n           ]},\n    },\n    'test': {\n       'class_name': 'albumentations.Compose',\n       'params': {\n           'transforms': [\n               {'class_name': 'albumentations.Resize', 'params': {'height': '${data.img_h}', 'width': '${data.img_w}', 'p': 1.0}},\n           ]},\n    },\n    'preprocessing': {\n        'class_name': 'albumentations.Compose',\n       'params': {\n           'transforms': [\n               {'class_name': 'albumentations.pytorch.transforms.ToTensorV2', 'params': {'p': 1.0, 'transpose_mask': True}}\n           ]},\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:28.247138Z","iopub.execute_input":"2024-10-09T11:10:28.247505Z","iopub.status.idle":"2024-10-09T11:10:28.259259Z","shell.execute_reply.started":"2024-10-09T11:10:28.247476Z","shell.execute_reply":"2024-10-09T11:10:28.25817Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dict_cfg['model'] = {\n    'class_name': 'segmentation_models_pytorch.Unet',\n    'params': {\n        'encoder_name': 'efficientnet-b3',\n        'encoder_weights': 'imagenet',\n        'in_channels': '${data.in_channels_num}',\n        'classes': '${data.classes_num}',\n        'activation': None,\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:32.08298Z","iopub.execute_input":"2024-10-09T11:10:32.08335Z","iopub.status.idle":"2024-10-09T11:10:32.088511Z","shell.execute_reply.started":"2024-10-09T11:10:32.083323Z","shell.execute_reply":"2024-10-09T11:10:32.087395Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dict_cfg['trainer'] = {\n    'accelerator': 'auto', 'devices': 1,\n    'max_epochs': 5, 'log_every_n_steps': 50,\n    'precision': '16-mixed'\n}\n\ndict_cfg['loggers'] = [\n    {'class_name': 'lightning.pytorch.loggers.WandbLogger',\n     'params': {\n         'save_dir': '${training.save_dir}', 'project': '${general.project_name}',\n         'name': '${training.save_name}', 'version': '${training.save_name}',\n         'entity': 'thdquan-no', 'log_model': False, \n         'notes': 'Adam lr 2e-3, CosineAnnealingLR 25, bs 128, size 224,\\\n         fold 0, copy augment - no normalize, unet with pretrained efficientnet-b3',\n     }\n    },\n]\n\ndict_cfg['callbacks'] = [\n    {'class_name': 'lightning.pytorch.callbacks.ModelCheckpoint',\n     'params': {\n         'mode': '${training.mode}', 'monitor': '${training.metric}',\n         'verbose': True, 'save_last': True, 'save_top_k': 2}\n    },\n    {'class_name': 'LitCheckpointTransferer',\n     'params': {\n         'artifact_name': 'lit_ckpts-${training.save_name}',\n         'artifact_alias': 'latest'}\n    },\n    {'class_name': 'lightning.pytorch.callbacks.LearningRateMonitor',\n     'params': {}\n    },\n]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:32.523967Z","iopub.execute_input":"2024-10-09T11:10:32.524342Z","iopub.status.idle":"2024-10-09T11:10:32.532042Z","shell.execute_reply.started":"2024-10-09T11:10:32.524312Z","shell.execute_reply":"2024-10-09T11:10:32.531085Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"dict_cfg['loss'] = {\n    'class_name': 'segmentation_models_pytorch.losses.FocalLoss',\n    'params': {\n        'mode': 'multilabel'\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:33.436122Z","iopub.execute_input":"2024-10-09T11:10:33.436931Z","iopub.status.idle":"2024-10-09T11:10:33.441469Z","shell.execute_reply.started":"2024-10-09T11:10:33.436896Z","shell.execute_reply":"2024-10-09T11:10:33.440337Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cfg = OmegaConf.create(dict_cfg)\nprint(OmegaConf.to_yaml(cfg))","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:34.228382Z","iopub.execute_input":"2024-10-09T11:10:34.229336Z","iopub.status.idle":"2024-10-09T11:10:34.27482Z","shell.execute_reply.started":"2024-10-09T11:10:34.229297Z","shell.execute_reply":"2024-10-09T11:10:34.273843Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"general:\n  workspace: kaggle\n  project_name: mri_seg\ntraining:\n  lr: 0.002\n  metric: val_GeneralizedDiceScore\n  seed: 42\n  debug: false\n  mode: max\n  save_dir: /kaggle/working\n  save_name: training_4\n  pretrained: false\n  pretrained_fname: ''\noptimizers:\n- class_name: torch.optim.AdamW\n  params:\n    lr: ${training.lr}\nschedulers:\n- scheduler:\n    class_name: torch.optim.lr_scheduler.CosineAnnealingLR\n    params:\n      T_max: 25\n      eta_min: 1.0e-05\n  interval: epoch\ndata:\n  full_image_path: /kaggle/working/full_image.csv\n  test_image_path: /kaggle/working/test_image.csv\n  num_workers: 4\n  batch_size: 128\n  in_channels_num: 3\n  img_h: 224\n  img_w: 224\n  classes:\n  - Large Bowel\n  - Small Bowel\n  - Stomach\n  classes_num: 3\n  splits_num: 5\n  fold_idx: 0\nmetrics:\n- class_name: torchmetrics.segmentation.GeneralizedDiceScore\n  params:\n    num_classes: ${data.classes_num}\n    weight_type: linear\naugmentation:\n  train:\n    class_name: albumentations.Compose\n    params:\n      transforms:\n      - class_name: albumentations.Resize\n        params:\n          height: ${data.img_h}\n          width: ${data.img_w}\n          p: 1.0\n      - class_name: albumentations.OneOf\n        params:\n          transforms:\n          - class_name: albumentations.Affine\n            params:\n              scale:\n              - -0.05\n              - 0.05\n              translate_percent:\n              - -0.05\n              - 0.05\n              rotate:\n              - -10\n              - 10\n          - class_name: albumentations.HorizontalFlip\n            params: {}\n          p: 0.5\n      - class_name: albumentations.OneOf\n        params:\n          transforms:\n          - class_name: albumentations.RandomBrightnessContrast\n            params:\n              brightness_limit: 0.2\n              contrast_limit: 0.2\n          p: 0.2\n      - class_name: albumentations.OneOf\n        params:\n          transforms:\n          - class_name: albumentations.GaussNoise\n            params:\n              var_limit:\n              - 10.0\n              - 50.0\n          p: 0.2\n  valid:\n    class_name: albumentations.Compose\n    params:\n      transforms:\n      - class_name: albumentations.Resize\n        params:\n          height: ${data.img_h}\n          width: ${data.img_w}\n          p: 1.0\n  test:\n    class_name: albumentations.Compose\n    params:\n      transforms:\n      - class_name: albumentations.Resize\n        params:\n          height: ${data.img_h}\n          width: ${data.img_w}\n          p: 1.0\n  preprocessing:\n    class_name: albumentations.Compose\n    params:\n      transforms:\n      - class_name: albumentations.pytorch.transforms.ToTensorV2\n        params:\n          p: 1.0\n          transpose_mask: true\nmodel:\n  class_name: segmentation_models_pytorch.Unet\n  params:\n    encoder_name: efficientnet-b3\n    encoder_weights: imagenet\n    in_channels: ${data.in_channels_num}\n    classes: ${data.classes_num}\n    activation: null\ntrainer:\n  accelerator: auto\n  devices: 1\n  max_epochs: 5\n  log_every_n_steps: 50\n  precision: 16-mixed\nloggers:\n- class_name: lightning.pytorch.loggers.WandbLogger\n  params:\n    save_dir: ${training.save_dir}\n    project: ${general.project_name}\n    name: ${training.save_name}\n    version: ${training.save_name}\n    entity: thdquan-no\n    log_model: false\n    notes: Adam lr 2e-3, CosineAnnealingLR 25, bs 128, size 224,         fold 0, copy\n      augment - no normalize, unet with pretrained efficientnet-b3\ncallbacks:\n- class_name: lightning.pytorch.callbacks.ModelCheckpoint\n  params:\n    mode: ${training.mode}\n    monitor: ${training.metric}\n    verbose: true\n    save_last: true\n    save_top_k: 2\n- class_name: LitCheckpointTransferer\n  params:\n    artifact_name: lit_ckpts-${training.save_name}\n    artifact_alias: latest\n- class_name: lightning.pytorch.callbacks.LearningRateMonitor\n  params: {}\nloss:\n  class_name: segmentation_models_pytorch.losses.FocalLoss\n  params:\n    mode: multilabel\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def load_obj(obj_path: str, default_obj_path: str = \"__main__\") -> Any:\n    \"\"\"Extract an object from a given path.\n        Args:\n            obj_path: Path to an object to be extracted, including the object name.\n            default_obj_path: Default object path.\n        Returns:\n            Extracted object.\n        Raises:\n            AttributeError: When the object does not have the given named attribute.\n    \"\"\"\n    obj_path_list = obj_path.rsplit(\".\", 1)\n    obj_path = obj_path_list.pop(0) if len(obj_path_list) > 1 else default_obj_path\n    obj_name = obj_path_list[0]\n    module_obj = importlib.import_module(obj_path)\n    if not hasattr(module_obj, obj_name):\n        raise AttributeError(\n            f\"Object `{obj_name}` cannot be loaded from `{obj_path}`.\"\n        )\n    return getattr(module_obj, obj_name)\n\ndef load_value(value_cfg: Union[Dict, List, Any]):\n    if type(value_cfg) == dict:\n        keys = list(value_cfg.keys())\n        if keys == ['class_name', 'params']:\n            return load_obj(value_cfg['class_name'])(**load_value(value_cfg['params']))\n        else:\n            return {k: load_value(v) for k, v in value_cfg.items()}\n    elif type(value_cfg) == list:\n        a= [load_value(v) for v in value_cfg]\n        return a\n    else:\n        return value_cfg \n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:35.198084Z","iopub.execute_input":"2024-10-09T11:10:35.199008Z","iopub.status.idle":"2024-10-09T11:10:35.208828Z","shell.execute_reply.started":"2024-10-09T11:10:35.198973Z","shell.execute_reply":"2024-10-09T11:10:35.20769Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def get_metadata(row):\n    data = row['id'].split('_')\n    case = int(data[0].replace('case',''))\n    day = int(data[1].replace('day',''))\n    slice_ = int(data[-1])\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n    return row\n\ndef path2info(row):\n    path = row['image_path']\n    data = path.split('/')\n    slice_ = int(data[-1].split('_')[1])\n    case = int(data[-3].split('_')[0].replace('case',''))\n    day = int(data[-3].split('_')[1].replace('day',''))\n    width = int(data[-1].split('_')[2])\n    height = int(data[-1].split('_')[3])\n    row['height'] = height\n    row['width'] = width\n    row['case'] = case\n    row['day'] = day\n    row['slice'] = slice_\n#     row['id'] = f'case{case}_day{day}_slice_{slice_}'\n    return row","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:35.500219Z","iopub.execute_input":"2024-10-09T11:10:35.501226Z","iopub.status.idle":"2024-10-09T11:10:35.512158Z","shell.execute_reply.started":"2024-10-09T11:10:35.501183Z","shell.execute_reply":"2024-10-09T11:10:35.511046Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def get_max_value(dtype):\n    if np.issubdtype(dtype, np.integer):\n        return float(np.iinfo(dtype).max)\n    elif np.issubdtype(dtype, np.floating):\n        return np.finfo(dtype).max\n    else:\n        raise TypeError(\"Unsupported data type\")\n        \ndef load_image(path):\n    image = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n    max_v = get_max_value(image.dtype)\n    image = image.astype('float32') # original is uint16\n    if max_v:\n        image /= max_v # scale image to [0, 1]\n    return image\n\ndef load_mask(path):\n    mask = np.load(path)\n    mask = mask.astype('float32')\n    mask /= 255.0\n    return mask\n\ndef show_img(img, mask=None):\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    img = clahe.apply(img)\n    plt.imshow(img, cmap='bone')\n    \n    if mask is not None:\n        # plt.imshow(np.ma.masked_where(mask!=1, mask), alpha=0.5, cmap='autumn')\n        plt.imshow(mask, alpha=0.5)\n        handles = [Rectangle((0,0),1,1, color=_c) for _c in [(0.667,0.0,0.0), (0.0,0.667,0.0), (0.0,0.0,0.667)]]\n        labels = [\"Large Bowel\", \"Small Bowel\", \"Stomach\"]\n        plt.legend(handles,labels)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:35.798285Z","iopub.execute_input":"2024-10-09T11:10:35.798956Z","iopub.status.idle":"2024-10-09T11:10:35.808961Z","shell.execute_reply.started":"2024-10-09T11:10:35.798922Z","shell.execute_reply":"2024-10-09T11:10:35.80804Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)  # Needed to align to RLE direction\n\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = img.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:36.08366Z","iopub.execute_input":"2024-10-09T11:10:36.084016Z","iopub.status.idle":"2024-10-09T11:10:36.092984Z","shell.execute_reply.started":"2024-10-09T11:10:36.08399Z","shell.execute_reply":"2024-10-09T11:10:36.092021Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/uwmgi-mask-dataset/train.csv')\ndf['segmentation'] = df.segmentation.fillna('')\ndf['rle_len'] = df.segmentation.map(len) # length of each rle mask\ndf['seg_num'] = df.segmentation.map(lambda x: 1 if x != '' else 0)\ndf['mask_path'] = df.mask_path.str.replace('/png/','/np').str.replace('.png','.npy')\n\ndf2 = df.groupby(['id'])['segmentation'].agg(list).to_frame().reset_index() # rle list of each id\ndf2 = df2.merge(df.groupby(['id'])['rle_len'].sum().to_frame().reset_index()) # total length of all rles of each id\ndf2 = df2.merge(df.groupby(['id'])['seg_num'].sum().to_frame().reset_index()) # total number of segments of each id\n\ndf = df.drop(columns=['segmentation', 'class', 'rle_len', 'seg_num'])\ndf = df.groupby(['id']).head(1).reset_index(drop=True)\ndf = df.merge(df2, on=['id'])\ndf['empty'] = (df.rle_len==0) # empty masks\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:36.748686Z","iopub.execute_input":"2024-10-09T11:10:36.749374Z","iopub.status.idle":"2024-10-09T11:10:40.117328Z","shell.execute_reply.started":"2024-10-09T11:10:36.749343Z","shell.execute_reply":"2024-10-09T11:10:40.116376Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                         id  case  day  slice  \\\n0  case123_day20_slice_0001   123   20      1   \n1  case123_day20_slice_0002   123   20      2   \n2  case123_day20_slice_0003   123   20      3   \n3  case123_day20_slice_0004   123   20      4   \n4  case123_day20_slice_0005   123   20      5   \n\n                                          image_path  height  width  \\\n0  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n1  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n2  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n3  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n4  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n\n                                           mask_path segmentation  rle_len  \\\n0  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...       [, , ]        0   \n1  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...       [, , ]        0   \n2  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...       [, , ]        0   \n3  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...       [, , ]        0   \n4  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...       [, , ]        0   \n\n   seg_num  empty  \n0        0   True  \n1        0   True  \n2        0   True  \n3        0   True  \n4        0   True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case</th>\n      <th>day</th>\n      <th>slice</th>\n      <th>image_path</th>\n      <th>height</th>\n      <th>width</th>\n      <th>mask_path</th>\n      <th>segmentation</th>\n      <th>rle_len</th>\n      <th>seg_num</th>\n      <th>empty</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>case123_day20_slice_0001</td>\n      <td>123</td>\n      <td>20</td>\n      <td>1</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>[, , ]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>case123_day20_slice_0002</td>\n      <td>123</td>\n      <td>20</td>\n      <td>2</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>[, , ]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>case123_day20_slice_0003</td>\n      <td>123</td>\n      <td>20</td>\n      <td>3</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>[, , ]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>case123_day20_slice_0004</td>\n      <td>123</td>\n      <td>20</td>\n      <td>4</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>[, , ]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>case123_day20_slice_0005</td>\n      <td>123</td>\n      <td>20</td>\n      <td>5</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>[, , ]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sns.countplot(df, x='seg_num')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:40.119047Z","iopub.execute_input":"2024-10-09T11:10:40.119354Z","iopub.status.idle":"2024-10-09T11:10:40.362594Z","shell.execute_reply.started":"2024-10-09T11:10:40.119329Z","shell.execute_reply":"2024-10-09T11:10:40.361682Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='seg_num', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create folds","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedGroupKFold\ndf['fold'] = -1\nskf = StratifiedGroupKFold(n_splits=cfg.data.splits_num, shuffle=True, random_state=cfg.training.seed)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df, df['seg_num'], groups = df[\"case\"])):\n    df.loc[val_idx, 'fold'] = int(fold)\n    \ndisplay(df.groupby(['fold', 'seg_num']).size())","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:41.076817Z","iopub.execute_input":"2024-10-09T11:10:41.077203Z","iopub.status.idle":"2024-10-09T11:10:41.240083Z","shell.execute_reply.started":"2024-10-09T11:10:41.077173Z","shell.execute_reply":"2024-10-09T11:10:41.239124Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"fold  seg_num\n0     0          4486\n      1           485\n      2          2230\n      3           719\n1     0          3847\n      1           347\n      2          1876\n      3           554\n2     0          4604\n      1           576\n      2          2138\n      3           634\n3     0          4862\n      1           734\n      2          2400\n      3           628\n4     0          4107\n      1           326\n      2          2277\n      3           666\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/full_image.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:41.336547Z","iopub.execute_input":"2024-10-09T11:10:41.337211Z","iopub.status.idle":"2024-10-09T11:10:42.925953Z","shell.execute_reply.started":"2024-10-09T11:10:41.337177Z","shell.execute_reply":"2024-10-09T11:10:42.925082Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class MRIDataset(Dataset):\n\n    def __init__(self, df, augmentation=None, preprocessing=None):\n        self.df = df\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n\n    def __len__(self):\n        return self.df.shape[0]\n    \n    def get_raw_item(self, idx):\n        image = load_image(self.df.loc[idx, \"image_path\"])\n        mask = load_mask(self.df.loc[idx, 'mask_path'])\n        return image, mask\n    \n    def __getitem__(self, idx):\n        image, mask = self.get_raw_item(idx)\n        if self.augmentation:\n            augmented = self.augmentation(image=image,mask=mask)\n            image, mask = augmented['image'], augmented['mask']\n            \n        if self.preprocessing:\n            preprocessed = self.preprocessing(image=image,mask=mask)\n            image, mask = preprocessed['image'], preprocessed['mask']\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:42.927393Z","iopub.execute_input":"2024-10-09T11:10:42.927725Z","iopub.status.idle":"2024-10-09T11:10:42.93612Z","shell.execute_reply.started":"2024-10-09T11:10:42.927699Z","shell.execute_reply":"2024-10-09T11:10:42.935091Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"def get_transform(augmentation_cfg):\n    augmentation_cfg = OmegaConf.to_container(augmentation_cfg, resolve=True)\n    transform = load_obj(augmentation_cfg['class_name'])(**load_value(augmentation_cfg['params']))\n    return transform","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:43.109295Z","iopub.execute_input":"2024-10-09T11:10:43.109668Z","iopub.status.idle":"2024-10-09T11:10:43.114859Z","shell.execute_reply.started":"2024-10-09T11:10:43.109626Z","shell.execute_reply":"2024-10-09T11:10:43.113862Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_augmentation = get_transform(cfg.augmentation.train)\nvalid_augmentation = get_transform(cfg.augmentation.valid)\npreprocessing = get_transform(cfg.augmentation.preprocessing)\nprint(f'\\n{train_augmentation}')\nprint(f'\\n{valid_augmentation}')\nprint(f'\\n{preprocessing}')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:43.718516Z","iopub.execute_input":"2024-10-09T11:10:43.719187Z","iopub.status.idle":"2024-10-09T11:10:43.737178Z","shell.execute_reply.started":"2024-10-09T11:10:43.719153Z","shell.execute_reply":"2024-10-09T11:10:43.736188Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"\nCompose([\n  Resize(always_apply=False, p=1.0, height=224, width=224, interpolation=1),\n  OneOf([\n    Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (-0.05, 0.05), 'y': (-0.05, 0.05)}, translate_percent={'x': (-0.05, 0.05), 'y': (-0.05, 0.05)}, translate_px=None, rotate=(-10, 10), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=False, rotate_method='largest_box'),\n    HorizontalFlip(always_apply=False, p=0.5),\n  ], p=0.5),\n  OneOf([\n    RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n  ], p=0.2),\n  OneOf([\n    GaussNoise(always_apply=False, p=0.5, var_limit=[10.0, 50.0], per_channel=True, mean=0),\n  ], p=0.2),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n\nCompose([\n  Resize(always_apply=False, p=1.0, height=224, width=224, interpolation=1),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n\nCompose([\n  ToTensorV2(always_apply=True, p=1.0, transpose_mask=True),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_augmentation = A.Compose([\n        A.Resize(224, 224, interpolation=cv2.INTER_NEAREST),\n        A.HorizontalFlip(p=0.5),\n#         A.VerticalFlip(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        A.OneOf([\n            A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n# #             A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        ], p=0.25),\n        A.CoarseDropout(max_holes=8, max_height=224//20, max_width=224//20,\n                         min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        ], p=1.0)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:44.172586Z","iopub.execute_input":"2024-10-09T11:10:44.172972Z","iopub.status.idle":"2024-10-09T11:10:44.180146Z","shell.execute_reply.started":"2024-10-09T11:10:44.172943Z","shell.execute_reply":"2024-10-09T11:10:44.17914Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Lightning Data","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\nclass LitMRIDataset(L.LightningDataModule):\n    def __init__(\n        self, data_cfg, training_cfg, **kwargs\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n    def prepare_data(self):\n        pass\n\n    def setup(self, stage: str):\n        if stage == \"fit\":\n            train_image_df = self.hparams.full_image_df[self.hparams.full_image_df['fold'] != self.hparams.data_cfg.fold_idx]\n            valid_image_df = self.hparams.full_image_df[self.hparams.full_image_df['fold'] == self.hparams.data_cfg.fold_idx]    \n            train_image_df.reset_index(drop=True, inplace=True)\n            valid_image_df.reset_index(drop=True, inplace=True)\n            \n            self.train_ds = MRIDataset(\n                train_image_df,\n                augmentation=self.hparams.train_augmentation,\n                preprocessing=self.hparams.preprocessing\n            )\n            self.valid_ds = MRIDataset(\n                valid_image_df,\n                augmentation=self.hparams.valid_augmentation,\n                preprocessing=self.hparams.preprocessing\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_ds, batch_size=self.hparams.data_cfg.batch_size,\n            shuffle=True, drop_last=False,\n            num_workers=self.hparams.data_cfg.num_workers,\n            pin_memory=True,\n            worker_init_fn=seed_worker,\n            generator=torch.Generator().manual_seed(self.hparams.training_cfg.seed),\\\n            persistent_workers=True,\n#             collate_fn=collate_fn\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.valid_ds, batch_size=self.hparams.data_cfg.batch_size,\n            shuffle=False, drop_last=False,\n            num_workers=self.hparams.data_cfg.num_workers,\n            pin_memory=True,\n            worker_init_fn=seed_worker,\n            generator=torch.Generator().manual_seed(self.hparams.training_cfg.seed),\\\n            persistent_workers=True,\n#             collate_fn=collate_fn\n        )","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:45.084955Z","iopub.execute_input":"2024-10-09T11:10:45.085666Z","iopub.status.idle":"2024-10-09T11:10:45.09726Z","shell.execute_reply.started":"2024-10-09T11:10:45.085608Z","shell.execute_reply":"2024-10-09T11:10:45.096309Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"full_image_df = pd.read_csv(cfg.data.full_image_path)\ndisplay(full_image_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:45.563965Z","iopub.execute_input":"2024-10-09T11:10:45.564662Z","iopub.status.idle":"2024-10-09T11:10:45.960472Z","shell.execute_reply.started":"2024-10-09T11:10:45.564606Z","shell.execute_reply":"2024-10-09T11:10:45.959516Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"                         id  case  day  slice  \\\n0  case123_day20_slice_0001   123   20      1   \n1  case123_day20_slice_0002   123   20      2   \n2  case123_day20_slice_0003   123   20      3   \n3  case123_day20_slice_0004   123   20      4   \n4  case123_day20_slice_0005   123   20      5   \n\n                                          image_path  height  width  \\\n0  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n1  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n2  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n3  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n4  /kaggle/input/uw-madison-gi-tract-image-segmen...     266    266   \n\n                                           mask_path  segmentation  rle_len  \\\n0  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...  ['', '', '']        0   \n1  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...  ['', '', '']        0   \n2  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...  ['', '', '']        0   \n3  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...  ['', '', '']        0   \n4  /kaggle/input/uwmgi-mask-dataset/np/uw-madison...  ['', '', '']        0   \n\n   seg_num  empty  fold  \n0        0   True     1  \n1        0   True     1  \n2        0   True     1  \n3        0   True     1  \n4        0   True     1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>case</th>\n      <th>day</th>\n      <th>slice</th>\n      <th>image_path</th>\n      <th>height</th>\n      <th>width</th>\n      <th>mask_path</th>\n      <th>segmentation</th>\n      <th>rle_len</th>\n      <th>seg_num</th>\n      <th>empty</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>case123_day20_slice_0001</td>\n      <td>123</td>\n      <td>20</td>\n      <td>1</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>['', '', '']</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>case123_day20_slice_0002</td>\n      <td>123</td>\n      <td>20</td>\n      <td>2</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>['', '', '']</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>case123_day20_slice_0003</td>\n      <td>123</td>\n      <td>20</td>\n      <td>3</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>['', '', '']</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>case123_day20_slice_0004</td>\n      <td>123</td>\n      <td>20</td>\n      <td>4</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>['', '', '']</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>case123_day20_slice_0005</td>\n      <td>123</td>\n      <td>20</td>\n      <td>5</td>\n      <td>/kaggle/input/uw-madison-gi-tract-image-segmen...</td>\n      <td>266</td>\n      <td>266</td>\n      <td>/kaggle/input/uwmgi-mask-dataset/np/uw-madison...</td>\n      <td>['', '', '']</td>\n      <td>0</td>\n      <td>0</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"lit_ds = LitMRIDataset(\n    cfg.data, cfg.training, \n    full_image_df=full_image_df,\n    train_augmentation=train_augmentation,\n    valid_augmentation=valid_augmentation,\n    preprocessing=preprocessing\n)\nprint(lit_ds.hparams.keys())\nlit_ds.prepare_data()\nlit_ds.setup('fit')","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:46.148711Z","iopub.execute_input":"2024-10-09T11:10:46.149078Z","iopub.status.idle":"2024-10-09T11:10:46.165782Z","shell.execute_reply.started":"2024-10-09T11:10:46.149051Z","shell.execute_reply":"2024-10-09T11:10:46.164701Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"dict_keys(['data_cfg', 'training_cfg', 'full_image_df', 'train_augmentation', 'valid_augmentation', 'preprocessing'])\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dl = lit_ds.train_dataloader()\nval_dl = lit_ds.val_dataloader()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:47.14852Z","iopub.execute_input":"2024-10-09T11:10:47.149264Z","iopub.status.idle":"2024-10-09T11:10:47.154239Z","shell.execute_reply.started":"2024-10-09T11:10:47.149229Z","shell.execute_reply":"2024-10-09T11:10:47.153119Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Lightning Model","metadata":{}},{"cell_type":"code","source":"def build_model(model_cfg):\n    model = load_obj(model_cfg.class_name)(**OmegaConf.to_container(model_cfg.params, resolve=True))\n#     model = smp.Unet(\n#         encoder_name='resnet50',      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n#         encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n#         in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n#         classes=3,        # model output channels (number of classes in your dataset)\n#         activation=None,\n#     )\n    return model\n\ndef create_example_input(data_cfg):\n    return torch.zeros((data_cfg.in_channels_num, data_cfg.img_h, data_cfg.img_w), dtype=torch.float32).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:49.01393Z","iopub.execute_input":"2024-10-09T11:10:49.014914Z","iopub.status.idle":"2024-10-09T11:10:49.02238Z","shell.execute_reply.started":"2024-10-09T11:10:49.014871Z","shell.execute_reply":"2024-10-09T11:10:49.021298Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class LitMRIModel(L.LightningModule):\n    def __init__(\n        self, model_cfg, optimizers_cfg,\n        metrics_cfg, data_cfg, schedulers_cfg,\n        loss_cfg\n    ):\n        super().__init__()\n        # Exports the hyperparameters to a YAML file, and create \"self.hparams\" namespace\n        self.save_hyperparameters()\n        # Create model\n        self.model = build_model(model_cfg)\n        # Example input for visualizing the graph in Tensorboard\n        self.example_input_array = create_example_input(data_cfg)\n        # Create loss\n        self.loss_module = load_obj(loss_cfg.class_name)(**OmegaConf.to_container(loss_cfg.params, resolve=True))\n        # Create metrics\n        self._configure_metrics()\n        \n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self.model(inputs)\n        loss = self.loss_module(outputs, targets)\n        # By default logs it per step and epoch\n        self.log(\"train_loss\", loss, logger=True)\n        \n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_idx):\n        inputs, targets = batch\n        outputs = self.model(inputs).detach()\n        preds = self.to_preds(outputs)\n#         print(f'targets shape: {target.shape}')\n#         print(f'preds shape: {preds.shape}')\n        \n        self.valid_metrics.update(preds, targets)\n\n    def on_validation_epoch_end(self):\n        val_metrics_value_dict = self.valid_metrics.compute()\n        self.log_dict(val_metrics_value_dict, logger=True, prog_bar=True)\n        # remember to reset metrics at the end of the epoch\n        self.valid_metrics.reset()\n            \n    def _configure_metrics(self):\n        metric_lst = [load_obj(i.class_name)(**OmegaConf.to_container(i.params, resolve=True)) for i in self.hparams.metrics_cfg]\n        metrics = torchmetrics.MetricCollection(metric_lst)\n        self.valid_metrics = metrics.clone(prefix='val_')\n        self.test_metrics = metrics.clone(prefix='test_')\n#         print(self.valid_metrics)\n\n    def configure_optimizers(self):\n        optimizer = load_obj(self.hparams.optimizers_cfg[0].class_name)(\n            filter(lambda p: p.requires_grad, self.model.parameters()),\n            **OmegaConf.to_container(self.hparams.optimizers_cfg[0].params, resolve=True)\n        )\n#         stepping_batches = self.trainer.estimated_stepping_batches\n#         print(f'##############################')\n#         print(stepping_batches)\n        scheduler = {\n            'scheduler':load_obj(self.hparams.schedulers_cfg[0].scheduler.class_name)(\n                optimizer,# total_steps=stepping_batches,\n                **OmegaConf.to_container(self.hparams.schedulers_cfg[0].scheduler.params, resolve=True)\n            ),\n            'interval': self.hparams.schedulers_cfg[0].interval\n        }     \n        optimizers, schedulers = [optimizer], [scheduler]\n        \n        return optimizers, schedulers\n    \n    def to_preds(self, outputs):\n        preds = outputs.sigmoid()\n        preds = torch.where(preds >= 0.5, 1.0, 0.0).to(torch.uint8)\n        return preds","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:49.584533Z","iopub.execute_input":"2024-10-09T11:10:49.585418Z","iopub.status.idle":"2024-10-09T11:10:49.600943Z","shell.execute_reply.started":"2024-10-09T11:10:49.585383Z","shell.execute_reply":"2024-10-09T11:10:49.599927Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def create_lit_model(lit_model_class, cfg, **kwargs):\n    if cfg.training.pretrained:\n        pretrained_fpath = os.path.join(cfg.training.save_dir, cfg.training.pretrained_fname)\n        model = lit_model_class.load_from_checkpoint(pretrained_fpath)\n    else:\n        model = lit_model_class(\n            cfg.model, cfg.optimizers, cfg.metrics,\n            cfg.data, cfg.schedulers, cfg.loss\n        )\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:50.027855Z","iopub.execute_input":"2024-10-09T11:10:50.028546Z","iopub.status.idle":"2024-10-09T11:10:50.034003Z","shell.execute_reply.started":"2024-10-09T11:10:50.02851Z","shell.execute_reply":"2024-10-09T11:10:50.033033Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model = create_lit_model(LitMRIModel, cfg) ","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:50.830172Z","iopub.execute_input":"2024-10-09T11:10:50.83054Z","iopub.status.idle":"2024-10-09T11:10:51.83828Z","shell.execute_reply.started":"2024-10-09T11:10:50.830511Z","shell.execute_reply":"2024-10-09T11:10:51.837261Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b3-5fb5a3c3.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b3-5fb5a3c3.pth\n100%|██████████| 47.1M/47.1M [00:00<00:00, 219MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"class LitCheckpointTransferer(Callback):\n    def __init__(self, artifact_name, artifact_alias):\n        super().__init__()\n        self.artifact_name = artifact_name\n        self.artifact_alias = artifact_alias\n\n    def on_fit_end(self, trainer, pl_module):\n        # Check for existing artifacts with the same name and delete them if necessary\n        run = wandb.run\n        try:\n            api = wandb.Api()\n            existing_artifacts = api.artifacts(\n                type_name='directory',\n                name=f\"{run.entity}/{run.project}/{self.artifact_name}\"\n            )\n\n            for artifact in existing_artifacts:\n                if artifact.type == \"directory\":\n                    artifact.delete(delete_aliases=True)\n            my_logger.info(f'Delete the existing artifacts')\n        except Exception as e:\n            my_logger.info(f'Error deleting checkpoint artifacts: {e}')\n\n        # Create a new artifact\n        ckpt_dirpath = logger_ = trainer.checkpoint_callback.dirpath\n        save_dir = trainer.loggers[0].save_dir\n        ckpt_rel_dirpath = os.path.relpath(ckpt_dirpath, save_dir)\n        artifact = wandb.Artifact(name=self.artifact_name, type=\"directory\")\n        artifact.add_dir(ckpt_dirpath, name=ckpt_rel_dirpath)\n        # Add files from the directory to the artifact\n        # for root, dirs, files in os.walk(directory_to_save):\n        #     for file in files:\n        #         file_path = os.path.join(root, file)\n        #         # Add file to the artifact\n        #         artifact.add_file(file_path, name=file_path)\n        #         print(file_path)\n        #         print(os.path.relpath(file_path, directory_to_save))\n\n        # Log the artifact\n        try:\n            wandb.log_artifact(artifact, aliases=self.artifact_alias)\n            my_logger.info(f\"Files uploaded from {ckpt_rel_dirpath}\")\n        except Exception as e:\n            my_logger.info(f\"Error uploading files: {e}\")\n            \ndef download_wandb_checkpoint_artifacts(\n    wandb_logger, artifact_name, artifact_alias, **kwargs\n):\n    \n    try:\n        save_dir = wandb_logger.save_dir\n        run = wandb_logger.experiment\n        # artifact = api.artifact(f\"{entity}/{project}/{artifact_name}:{alias}\")\n        artifact = run.use_artifact(f\"{artifact_name}:{artifact_alias}\", type='directory')\n        artifact_dir = artifact.download(save_dir)\n        my_logger.info(f\"Checkpoint downloaded to {artifact_dir}\")\n    except Exception as e:\n        my_logger.info(f\"Error downloading files: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:53.153191Z","iopub.execute_input":"2024-10-09T11:10:53.154087Z","iopub.status.idle":"2024-10-09T11:10:53.166419Z","shell.execute_reply.started":"2024-10-09T11:10:53.154052Z","shell.execute_reply":"2024-10-09T11:10:53.16441Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def create_trainer(trainer_cfg, callbacks_cfg, loggers_cfg):\n    # Create a PyTorch Lightning trainer with the generation callback\n    logger_lst = [load_obj(i.class_name)(**OmegaConf.to_container(i.params, resolve=True)) for i in loggers_cfg]\n    callback_lst = [load_obj(i.class_name)(**OmegaConf.to_container(i.params, resolve=True)) for i in callbacks_cfg]\n    trainer = L.Trainer(\n        logger=logger_lst,\n        callbacks=callback_lst,\n        **trainer_cfg\n    )\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:53.605077Z","iopub.execute_input":"2024-10-09T11:10:53.605444Z","iopub.status.idle":"2024-10-09T11:10:53.611365Z","shell.execute_reply.started":"2024-10-09T11:10:53.605414Z","shell.execute_reply":"2024-10-09T11:10:53.610417Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"cfg.trainer.max_epochs = 20","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:54.373563Z","iopub.execute_input":"2024-10-09T11:10:54.374232Z","iopub.status.idle":"2024-10-09T11:10:54.378544Z","shell.execute_reply.started":"2024-10-09T11:10:54.374197Z","shell.execute_reply":"2024-10-09T11:10:54.377669Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"trainer = create_trainer(cfg.trainer, cfg.callbacks, cfg.loggers)\ndownload_wandb_checkpoint_artifacts(\n    wandb_logger=trainer.loggers[0], \n    artifact_name = cfg.callbacks[1].params.artifact_name,\n    artifact_alias = cfg.callbacks[1].params.artifact_alias\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:10:55.20576Z","iopub.execute_input":"2024-10-09T11:10:55.206754Z","iopub.status.idle":"2024-10-09T11:11:03.662202Z","shell.execute_reply.started":"2024-10-09T11:10:55.206705Z","shell.execute_reply":"2024-10-09T11:11:03.661235Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"INFO: Using 16bit Automatic Mixed Precision (AMP)\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthdquan\u001b[0m (\u001b[33mthdquan-no\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241009_111057-training_4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Resuming run <strong><a href='https://wandb.ai/thdquan-no/mri_seg/runs/training_4' target=\"_blank\">training_4</a></strong> to <a href='https://wandb.ai/thdquan-no/mri_seg' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/thdquan-no/mri_seg' target=\"_blank\">https://wandb.ai/thdquan-no/mri_seg</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/thdquan-no/mri_seg/runs/training_4' target=\"_blank\">https://wandb.ai/thdquan-no/mri_seg/runs/training_4</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact lit_ckpts-training_4:latest, 440.89MB. 3 files... \n\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \nDone. 0:0:3.4\n","output_type":"stream"},{"name":"stdout","text":"2024-10-09 11:11:03,656 - my_logger - INFO - Checkpoint downloaded to /kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(\n    model, train_dataloaders=train_dl,\n    val_dataloaders=val_dl, ckpt_path=\"last\")","metadata":{"execution":{"iopub.status.busy":"2024-10-09T11:11:03.664619Z","iopub.execute_input":"2024-10-09T11:11:03.665006Z","iopub.status.idle":"2024-10-09T12:06:50.934899Z","shell.execute_reply.started":"2024-10-09T11:11:03.66497Z","shell.execute_reply":"2024-10-09T12:06:50.933949Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/mri_seg/training_4/checkpoints exists and is not empty.\nINFO: Restoring states from the checkpoint path at /kaggle/working/mri_seg/training_4/checkpoints/last.ckpt\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name          | Type             | Params | Mode  | In sizes         | Out sizes       \n-------------------------------------------------------------------------------------------------\n0 | model         | Unet             | 13.2 M | train | [1, 3, 224, 224] | [1, 3, 224, 224]\n1 | loss_module   | FocalLoss        | 0      | train | ?                | ?               \n2 | valid_metrics | MetricCollection | 0      | train | ?                | ?               \n3 | test_metrics  | MetricCollection | 0      | train | ?                | ?               \n-------------------------------------------------------------------------------------------------\n13.2 M    Trainable params\n0         Non-trainable params\n13.2 M    Total params\n52.637    Total estimated model params size (MB)\n474       Modules in train mode\n0         Modules in eval mode\nINFO: Restored all states from the checkpoint at /kaggle/working/mri_seg/training_4/checkpoints/last.ckpt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344a4a904c8c41d088901d2784959441"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 10, global step 2629: 'val_GeneralizedDiceScore' was not in top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 11, global step 2868: 'val_GeneralizedDiceScore' was not in top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 12, global step 3107: 'val_GeneralizedDiceScore' was not in top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 13, global step 3346: 'val_GeneralizedDiceScore' reached 0.31749 (best 0.31749), saving model to '/kaggle/working/mri_seg/training_4/checkpoints/epoch=13-step=3346.ckpt' as top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 14, global step 3585: 'val_GeneralizedDiceScore' reached 0.31598 (best 0.31749), saving model to '/kaggle/working/mri_seg/training_4/checkpoints/epoch=14-step=3585.ckpt' as top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 15, global step 3824: 'val_GeneralizedDiceScore' reached 0.32232 (best 0.32232), saving model to '/kaggle/working/mri_seg/training_4/checkpoints/epoch=15-step=3824.ckpt' as top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 16, global step 4063: 'val_GeneralizedDiceScore' reached 0.31971 (best 0.32232), saving model to '/kaggle/working/mri_seg/training_4/checkpoints/epoch=16-step=4063.ckpt' as top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 17, global step 4302: 'val_GeneralizedDiceScore' was not in top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 18, global step 4541: 'val_GeneralizedDiceScore' was not in top 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: Epoch 19, global step 4780: 'val_GeneralizedDiceScore' was not in top 2\nINFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n","output_type":"stream"},{"name":"stdout","text":"2024-10-09 12:06:49,868 - my_logger - INFO - Delete the existing artifacts\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working/mri_seg/training_4/checkpoints)... Done. 1.0s\n","output_type":"stream"},{"name":"stdout","text":"2024-10-09 12:06:50,928 - my_logger - INFO - Files uploaded from mri_seg/training_4/checkpoints\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Extract model's the best weight","metadata":{}},{"cell_type":"code","source":"# !rm -rf /kaggle/working/mri_seg/*","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:21:54.808958Z","iopub.execute_input":"2024-10-09T12:21:54.809372Z","iopub.status.idle":"2024-10-09T12:21:54.815344Z","shell.execute_reply.started":"2024-10-09T12:21:54.809334Z","shell.execute_reply":"2024-10-09T12:21:54.814278Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from collections import OrderedDict\n\ndef extract_best_weight(trainer):\n    best_lit_ckpt = torch.load(trainer.checkpoint_callback.state_dict()['best_model_path'])\n    best_lit_weight = best_lit_ckpt['state_dict']\n    \n    # Transform lit model's weight to torch model's weight\n    key_mappings = {k: k[6:] for k in best_lit_weight.keys()}\n    best_torch_weight = OrderedDict()\n    \n    for old_k, new_k in key_mappings.items():\n        if old_k in best_lit_weight:\n            best_torch_weight[new_k] = best_lit_weight[old_k]\n    return best_torch_weight","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:21:55.118198Z","iopub.execute_input":"2024-10-09T12:21:55.11856Z","iopub.status.idle":"2024-10-09T12:21:55.126653Z","shell.execute_reply.started":"2024-10-09T12:21:55.118529Z","shell.execute_reply":"2024-10-09T12:21:55.125288Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"best_torch_w = extract_best_weight(trainer)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:21:56.215823Z","iopub.execute_input":"2024-10-09T12:21:56.21621Z","iopub.status.idle":"2024-10-09T12:21:56.544016Z","shell.execute_reply.started":"2024-10-09T12:21:56.216179Z","shell.execute_reply":"2024-10-09T12:21:56.543047Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"best_w_path = '/kaggle/working/unet_efficientnet.pt'\ntorch.save(best_torch_w, best_w_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:22:01.649614Z","iopub.execute_input":"2024-10-09T12:22:01.650542Z","iopub.status.idle":"2024-10-09T12:22:01.797412Z","shell.execute_reply.started":"2024-10-09T12:22:01.650502Z","shell.execute_reply":"2024-10-09T12:22:01.796311Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"wandb.finish()\n!rm -rf ./wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:23:54.127619Z","iopub.execute_input":"2024-10-09T12:23:54.128044Z","iopub.status.idle":"2024-10-09T12:23:59.341776Z","shell.execute_reply.started":"2024-10-09T12:23:54.128013Z","shell.execute_reply":"2024-10-09T12:23:59.340402Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='440.910 MB of 440.910 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>lr-AdamW</td><td>█▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train_loss</td><td>▆▇▅▃█▄▆█▃▄▇▇▆▄▄▄▄▃▂▅▄▄▂▄▂▂▅▄▁▃▅▅▃▄▃▆▂▄▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>val_GeneralizedDiceScore</td><td>▆▁▇█▇██▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-AdamW</td><td>0.00028</td></tr><tr><td>train_loss</td><td>0.00106</td></tr><tr><td>trainer/global_step</td><td>4779</td></tr><tr><td>val_GeneralizedDiceScore</td><td>0.31824</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">training_4</strong> at: <a href='https://wandb.ai/thdquan-no/mri_seg/runs/training_4' target=\"_blank\">https://wandb.ai/thdquan-no/mri_seg/runs/training_4</a><br/> View project at: <a href='https://wandb.ai/thdquan-no/mri_seg' target=\"_blank\">https://wandb.ai/thdquan-no/mri_seg</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241009_111057-training_4/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}